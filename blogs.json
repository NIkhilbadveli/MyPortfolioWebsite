{"status":"ok","feed":{"url":"https://medium.com/feed/@nikhilbadveli6","title":"Stories by Nikhil Badveli on Medium","link":"https://medium.com/@nikhilbadveli6?source=rss-9059cc379720------2","author":"","description":"Stories by Nikhil Badveli on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/2*Lu2Oc2tJLMoBxSn1j_-8xA.jpeg"},"items":[{"title":"Did DeepMind crack the surface of Artificial General Intelligence (AGI)?","pubDate":"2022-05-16 12:13:04","link":"https://medium.com/@nikhilbadveli6/did-deepmind-crack-the-surface-of-artificial-general-intelligence-agi-5cb878611c28?source=rss-9059cc379720------2","guid":"https://medium.com/p/5cb878611c28","author":"Nikhil Badveli","thumbnail":"https://cdn-images-1.medium.com/max/772/1*BWh6IpTSZYUM8Hq396FX3g.jpeg","description":"\n<p>DeepMind, yet again published a crazy paper and this time it involves a robot in a virtual environment behaving according to human instructions expressed in natural language through a chat like interface. See the below video for yourself and judge how good it\u00a0is!</p>\n<a href=\"https://medium.com/media/7ef754b659979610e469c16ee88dbe66/href\">https://medium.com/media/7ef754b659979610e469c16ee88dbe66/href</a><h3>AGI\u2026 the ultimate dream of every AI researcher</h3>\n<p>Artificial General Intelligence (AGI), the holy grail of the modern AI research is when we reach a state where the AI is capable of learning any previously unseen task without requiring large amount of data and time for training, and eventually become better at it than\u00a0humans.</p>\n<blockquote>Essentially, a very powerful generalized learning machine, hopefully without its own schemes of taking over the world. <em>Just kidding\u00a0:)</em>\n</blockquote>\n<p>After watching the above demo, it really felt like DeepMind had created a very primitive version of an AGI. Maybe a couple of papers down the line, we could see a more capable and advanced version of this\u00a0agent.</p>\n<h3>So, what exactly is there in this\u00a0paper?</h3>\n<p>The researchers have created a virtual environment of a model <strong>playhouse </strong>with the usual rooms you see in a typical house, like a bedroom, bathroom, hall etc., And they have randomized the layout of this playhouse for every time they test the capability of the\u00a0agent.</p>\n<p>Speaking of the agent itself, they\u2019ve named it <strong>Multimodal Interactive Agent</strong> (MIA)\u2026 let\u2019s call the agent Mia from now on. Mia is capable of interacting with the users/humans through two modes\u200a\u2014\u200aVisual and Textual. Mia is so amazing at recognizing objects in its field of vision that it can answer questions about what it sees presently.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/772/1*BWh6IpTSZYUM8Hq396FX3g.jpeg\"><figcaption>Example tasks from the paper. Image taken from DeepMind\u2019s blog.</figcaption></figure><p>See the third task from the above image and you\u2019ll be shocked how good it is at answering questions like these. Some of the other tasks, the researchers asked it to do are\u200a\u2014\u200atidying up the room, picking up objects, placing them at a different place\u00a0etc.,</p>\n<p>What\u2019s more amazing is that it takes just a few hours for Mia to learn about a previously unseen object or an unseen command. Essentially, in these few hours Mia is able to learn to recognize the new object, manipulate it according to the task and answer any questions about it. Such rich learning behavior!</p>\n<h3>What can we expect from this in the\u00a0future?</h3>\n<p>Whatever the pessimists might say about bringing AGI into life, I\u2019m still very optimistic and looking forward to see this possibility come to fruition. Obviously, there\u2019s still a ton of work to be done before we get to see that\u00a0day.</p>\n<p>I\u2019m not saying that this paper is the foundation for achieving AGI. Maybe they\u2019re showing us hand picked examples of tasks where Mia performed well\u2026 maybe there are still some drawbacks the researchers haven\u2019t thoroughly tested. But, this is a continuous process and we have to appreciate the amazing progress we have made\u00a0already.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gKF9j7-wJGdybPl6uCea3A.jpeg\"><figcaption>Red Dead Redemption 2\u200a\u2014\u200aa massive Open World game by Rockstar\u00a0Games.</figcaption></figure><p>One possibility I always had in my mind was teaching an agent like Mia in an Open World Game setting. I know it is computationally a lot more intensive than learning inside a toy playhouse, but maybe in a couple of years this might be possible. Who knows how much more Mia could do\u00a0then.</p>\n<p>As the Prof. K\u00e1roly Zsolnai-Feh\u00e9r from <a href=\"https://www.youtube.com/c/K%C3%A1rolyZsolnai\">Two Minute Papers</a> say, \u201c<strong><em>what a time to be\u00a0alive!</em></strong>\u201d.</p>\n<h3>References</h3>\n<p><a href=\"https://arxiv.org/abs/2112.03763\">Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning</a></p>\n<a href=\"https://medium.com/media/6e3ad22ac95821c66a99e61e19fc8d1d/href\">https://medium.com/media/6e3ad22ac95821c66a99e61e19fc8d1d/href</a><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5cb878611c28\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>DeepMind, yet again published a crazy paper and this time it involves a robot in a virtual environment behaving according to human instructions expressed in natural language through a chat like interface. See the below video for yourself and judge how good it\u00a0is!</p>\n<a href=\"https://medium.com/media/7ef754b659979610e469c16ee88dbe66/href\">https://medium.com/media/7ef754b659979610e469c16ee88dbe66/href</a><h3>AGI\u2026 the ultimate dream of every AI researcher</h3>\n<p>Artificial General Intelligence (AGI), the holy grail of the modern AI research is when we reach a state where the AI is capable of learning any previously unseen task without requiring large amount of data and time for training, and eventually become better at it than\u00a0humans.</p>\n<blockquote>Essentially, a very powerful generalized learning machine, hopefully without its own schemes of taking over the world. <em>Just kidding\u00a0:)</em>\n</blockquote>\n<p>After watching the above demo, it really felt like DeepMind had created a very primitive version of an AGI. Maybe a couple of papers down the line, we could see a more capable and advanced version of this\u00a0agent.</p>\n<h3>So, what exactly is there in this\u00a0paper?</h3>\n<p>The researchers have created a virtual environment of a model <strong>playhouse </strong>with the usual rooms you see in a typical house, like a bedroom, bathroom, hall etc., And they have randomized the layout of this playhouse for every time they test the capability of the\u00a0agent.</p>\n<p>Speaking of the agent itself, they\u2019ve named it <strong>Multimodal Interactive Agent</strong> (MIA)\u2026 let\u2019s call the agent Mia from now on. Mia is capable of interacting with the users/humans through two modes\u200a\u2014\u200aVisual and Textual. Mia is so amazing at recognizing objects in its field of vision that it can answer questions about what it sees presently.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/772/1*BWh6IpTSZYUM8Hq396FX3g.jpeg\"><figcaption>Example tasks from the paper. Image taken from DeepMind\u2019s blog.</figcaption></figure><p>See the third task from the above image and you\u2019ll be shocked how good it is at answering questions like these. Some of the other tasks, the researchers asked it to do are\u200a\u2014\u200atidying up the room, picking up objects, placing them at a different place\u00a0etc.,</p>\n<p>What\u2019s more amazing is that it takes just a few hours for Mia to learn about a previously unseen object or an unseen command. Essentially, in these few hours Mia is able to learn to recognize the new object, manipulate it according to the task and answer any questions about it. Such rich learning behavior!</p>\n<h3>What can we expect from this in the\u00a0future?</h3>\n<p>Whatever the pessimists might say about bringing AGI into life, I\u2019m still very optimistic and looking forward to see this possibility come to fruition. Obviously, there\u2019s still a ton of work to be done before we get to see that\u00a0day.</p>\n<p>I\u2019m not saying that this paper is the foundation for achieving AGI. Maybe they\u2019re showing us hand picked examples of tasks where Mia performed well\u2026 maybe there are still some drawbacks the researchers haven\u2019t thoroughly tested. But, this is a continuous process and we have to appreciate the amazing progress we have made\u00a0already.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gKF9j7-wJGdybPl6uCea3A.jpeg\"><figcaption>Red Dead Redemption 2\u200a\u2014\u200aa massive Open World game by Rockstar\u00a0Games.</figcaption></figure><p>One possibility I always had in my mind was teaching an agent like Mia in an Open World Game setting. I know it is computationally a lot more intensive than learning inside a toy playhouse, but maybe in a couple of years this might be possible. Who knows how much more Mia could do\u00a0then.</p>\n<p>As the Prof. K\u00e1roly Zsolnai-Feh\u00e9r from <a href=\"https://www.youtube.com/c/K%C3%A1rolyZsolnai\">Two Minute Papers</a> say, \u201c<strong><em>what a time to be\u00a0alive!</em></strong>\u201d.</p>\n<h3>References</h3>\n<p><a href=\"https://arxiv.org/abs/2112.03763\">Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning</a></p>\n<a href=\"https://medium.com/media/6e3ad22ac95821c66a99e61e19fc8d1d/href\">https://medium.com/media/6e3ad22ac95821c66a99e61e19fc8d1d/href</a><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5cb878611c28\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["agi","deepmind","reinforcement-learning","multimodal-learning"]},{"title":"Chatbot\u200a\u2014\u200a1, Natural Language Processing (NLP)","pubDate":"2022-05-11 12:27:02","link":"https://medium.com/@nikhilbadveli6/chatbot-1-natural-language-processing-nlp-36d510477ef1?source=rss-9059cc379720------2","guid":"https://medium.com/p/36d510477ef1","author":"Nikhil Badveli","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*wAR3r0zwrYRQ6VMNJCSltg.png","description":"\n<h3>Chatbot\u200a\u2014\u200a1, Natural Language Processing (NLP)</h3>\n<p>This is the first post in the chatbot series, which itself is part of a bigger series on NLP. I\u2019m not going to explain the basics of NLP, a quick google search will point you to an infinite amount of resources on that. Like the below article, for instance.</p>\n<p><a href=\"https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863\">A Gentle Introduction to Natural Language Processing</a></p>\n<h3>Introduction</h3>\n<p>Chatbots are an interesting application in the NLP domain. I mean, who doesn\u2019t want to feel like they\u2019re talking to an intelligent AI? But, did you know that there are actually two very different type of chatbots.</p>\n<ol>\n<li>One is called a <strong><em>retrieval-based chatbot</em> </strong>that can only output from a fixed set of responses. Essentially, we try to classify the user input into a set of pre-defined intent tags (ex: greeting, thanks etc.,) and choose a response at random, based on the intent we think the user has expressed.</li>\n<li>The other is called a <strong><em>generative chatbot </em></strong>that uses a seq2seq model or an encoder-decoder architecture in its most general form. First the user input<br>is encoded and then the decoder takes this encoded output, processes it and produces the output response of the\u00a0chatbot.</li>\n</ol>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wAR3r0zwrYRQ6VMNJCSltg.png\"><figcaption>A visual overview of a retreival based chatbot. Image taken from Drift\u00a0blog.</figcaption></figure><blockquote>Surprisingly, it looks like the popular digital assistants like Google Assistant, Siri etc., uses the first approach. And the second type is apparently still under research phase. I guess there\u2019s more control in the first type, at least if you consider business\u00a0needs.</blockquote>\n<p>Anyways, for this post we\u2019re going to implement the first type of the chatbot, which will support the customers in answering their queries related to food delivery.</p>\n<h3>Training the intents\u00a0data</h3>\n<p>We need to build a multi-class classifier that can predict the intent tag given an user input. We could choose any classic ML classifiers like SVM, kNN, Naive-bayes. But, we will use a simple neural network based model with 2 hidden\u00a0layers.</p>\n<p>Let\u2019s look at how the training data (<em>.json file</em>) looks\u00a0like:</p>\n<a href=\"https://medium.com/media/45d972dfd4d44f50a973fe60c5a0aed9/href\">https://medium.com/media/45d972dfd4d44f50a973fe60c5a0aed9/href</a><p>As you can see, each item in the list has a <em>tag</em>, a list of <em>patterns, </em>and a list of <em>responses </em>to choose from. For the classifier our input is the patterns data and the output classes are the\u00a0tags.</p>\n<h4>Preprocessing the\u00a0data</h4>\n<p>We can\u2019t directly feed this to the neural network since this is text data. We need to preprocess this data, which involves removing stop words, lemmatizing etc., The below code uses the popular NLTK package for cleaning the\u00a0data.</p>\n<a href=\"https://medium.com/media/1359f35bc55f0062f8a0d208681036a0/href\">https://medium.com/media/1359f35bc55f0062f8a0d208681036a0/href</a><p>Now, let\u2019s use a <em>Tf-Idf vectorizer </em>to transform this cleaned data into a set of vectors and a one hot encoder for the output class\u00a0labels.</p>\n<a href=\"https://medium.com/media/70269e4372a28bf766ca9d0ad428f225/href\">https://medium.com/media/70269e4372a28bf766ca9d0ad428f225/href</a><h4>Build and fit the\u00a0model</h4>\n<p>We will use the <em>keras </em>library that uses tensorflow backend for building our neural network model. We will add 2 hidden layers with ReLU activation and an output layer with a Softmax activation and a couple of dropout layers for regularization.</p>\n<a href=\"https://medium.com/media/66863387434eca82345063060a94dc6e/href\">https://medium.com/media/66863387434eca82345063060a94dc6e/href</a><p>Since our data is very small, I\u2019ve used a <em>batch_size</em> of 1, choose this depending on your data\u2019s size. Similarly, for other hyperparameters.</p>\n<p>Hooray! Now we have a classifier that can tell the intent of the user (hopefully). The next step is to use this model and put together everything else to generate a response.</p>\n<h3>Chatting loop</h3>\n<p>We will get the user input in an infinite loop fashion, generating response for every query the user has. Once we get the input text from the user, we have to preprocess it in exactly the same way as before we did while training.</p>\n<p>And then give this to the model, which generates probabilities for all the output intent tags, out of which we select the one with the highest probability.</p>\n<a href=\"https://medium.com/media/1387f9f83ff9422e56b1d2f6fcefe135/href\">https://medium.com/media/1387f9f83ff9422e56b1d2f6fcefe135/href</a><p>Note that we used a helper function <em>get_intent </em>that gives the corresponding intent to the predicted tag and we are using this intent to randomly select the response. In the case of a <em>goodbye</em> intent, we are breaking out of the chat\u00a0loop.</p>\n<p>That\u2019s all folks, you\u2019ve successfully built a retrieval based chatbot. I\u2019m sure you might see the bot giving repeated responses which is pretty annoying I know. Wait for me to post about the <strong>generative chatbot</strong> and you\u2019ll be amused by its responses.</p>\n<p>If you\u2019re interested in getting started with <strong>Reinforcement learning</strong>, read my articles on the\u00a0same.</p>\n<ul>\n<li><a href=\"https://medium.com/@nikhilbadveli6/reinforcement-learning-1-79cdec3599f9\">Reinforcement Learning\u200a\u2014\u200a1</a></li>\n<li><a href=\"https://medium.com/@nikhilbadveli6/reinforcement-learning-2-8e525986b3df\">Reinforcement Learning\u200a\u2014\u200a2</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=36d510477ef1\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Chatbot\u200a\u2014\u200a1, Natural Language Processing (NLP)</h3>\n<p>This is the first post in the chatbot series, which itself is part of a bigger series on NLP. I\u2019m not going to explain the basics of NLP, a quick google search will point you to an infinite amount of resources on that. Like the below article, for instance.</p>\n<p><a href=\"https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863\">A Gentle Introduction to Natural Language Processing</a></p>\n<h3>Introduction</h3>\n<p>Chatbots are an interesting application in the NLP domain. I mean, who doesn\u2019t want to feel like they\u2019re talking to an intelligent AI? But, did you know that there are actually two very different type of chatbots.</p>\n<ol>\n<li>One is called a <strong><em>retrieval-based chatbot</em> </strong>that can only output from a fixed set of responses. Essentially, we try to classify the user input into a set of pre-defined intent tags (ex: greeting, thanks etc.,) and choose a response at random, based on the intent we think the user has expressed.</li>\n<li>The other is called a <strong><em>generative chatbot </em></strong>that uses a seq2seq model or an encoder-decoder architecture in its most general form. First the user input<br>is encoded and then the decoder takes this encoded output, processes it and produces the output response of the\u00a0chatbot.</li>\n</ol>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*wAR3r0zwrYRQ6VMNJCSltg.png\"><figcaption>A visual overview of a retreival based chatbot. Image taken from Drift\u00a0blog.</figcaption></figure><blockquote>Surprisingly, it looks like the popular digital assistants like Google Assistant, Siri etc., uses the first approach. And the second type is apparently still under research phase. I guess there\u2019s more control in the first type, at least if you consider business\u00a0needs.</blockquote>\n<p>Anyways, for this post we\u2019re going to implement the first type of the chatbot, which will support the customers in answering their queries related to food delivery.</p>\n<h3>Training the intents\u00a0data</h3>\n<p>We need to build a multi-class classifier that can predict the intent tag given an user input. We could choose any classic ML classifiers like SVM, kNN, Naive-bayes. But, we will use a simple neural network based model with 2 hidden\u00a0layers.</p>\n<p>Let\u2019s look at how the training data (<em>.json file</em>) looks\u00a0like:</p>\n<a href=\"https://medium.com/media/45d972dfd4d44f50a973fe60c5a0aed9/href\">https://medium.com/media/45d972dfd4d44f50a973fe60c5a0aed9/href</a><p>As you can see, each item in the list has a <em>tag</em>, a list of <em>patterns, </em>and a list of <em>responses </em>to choose from. For the classifier our input is the patterns data and the output classes are the\u00a0tags.</p>\n<h4>Preprocessing the\u00a0data</h4>\n<p>We can\u2019t directly feed this to the neural network since this is text data. We need to preprocess this data, which involves removing stop words, lemmatizing etc., The below code uses the popular NLTK package for cleaning the\u00a0data.</p>\n<a href=\"https://medium.com/media/1359f35bc55f0062f8a0d208681036a0/href\">https://medium.com/media/1359f35bc55f0062f8a0d208681036a0/href</a><p>Now, let\u2019s use a <em>Tf-Idf vectorizer </em>to transform this cleaned data into a set of vectors and a one hot encoder for the output class\u00a0labels.</p>\n<a href=\"https://medium.com/media/70269e4372a28bf766ca9d0ad428f225/href\">https://medium.com/media/70269e4372a28bf766ca9d0ad428f225/href</a><h4>Build and fit the\u00a0model</h4>\n<p>We will use the <em>keras </em>library that uses tensorflow backend for building our neural network model. We will add 2 hidden layers with ReLU activation and an output layer with a Softmax activation and a couple of dropout layers for regularization.</p>\n<a href=\"https://medium.com/media/66863387434eca82345063060a94dc6e/href\">https://medium.com/media/66863387434eca82345063060a94dc6e/href</a><p>Since our data is very small, I\u2019ve used a <em>batch_size</em> of 1, choose this depending on your data\u2019s size. Similarly, for other hyperparameters.</p>\n<p>Hooray! Now we have a classifier that can tell the intent of the user (hopefully). The next step is to use this model and put together everything else to generate a response.</p>\n<h3>Chatting loop</h3>\n<p>We will get the user input in an infinite loop fashion, generating response for every query the user has. Once we get the input text from the user, we have to preprocess it in exactly the same way as before we did while training.</p>\n<p>And then give this to the model, which generates probabilities for all the output intent tags, out of which we select the one with the highest probability.</p>\n<a href=\"https://medium.com/media/1387f9f83ff9422e56b1d2f6fcefe135/href\">https://medium.com/media/1387f9f83ff9422e56b1d2f6fcefe135/href</a><p>Note that we used a helper function <em>get_intent </em>that gives the corresponding intent to the predicted tag and we are using this intent to randomly select the response. In the case of a <em>goodbye</em> intent, we are breaking out of the chat\u00a0loop.</p>\n<p>That\u2019s all folks, you\u2019ve successfully built a retrieval based chatbot. I\u2019m sure you might see the bot giving repeated responses which is pretty annoying I know. Wait for me to post about the <strong>generative chatbot</strong> and you\u2019ll be amused by its responses.</p>\n<p>If you\u2019re interested in getting started with <strong>Reinforcement learning</strong>, read my articles on the\u00a0same.</p>\n<ul>\n<li><a href=\"https://medium.com/@nikhilbadveli6/reinforcement-learning-1-79cdec3599f9\">Reinforcement Learning\u200a\u2014\u200a1</a></li>\n<li><a href=\"https://medium.com/@nikhilbadveli6/reinforcement-learning-2-8e525986b3df\">Reinforcement Learning\u200a\u2014\u200a2</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=36d510477ef1\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["chatbots","retrieval","intent","nlp"]},{"title":"Dumbest crypto trading strategy ever!","pubDate":"2022-05-09 13:56:34","link":"https://wire.insiderfinance.io/dumbest-crypto-trading-strategy-ever-fba2f11f5073?source=rss-9059cc379720------2","guid":"https://medium.com/p/fba2f11f5073","author":"Nikhil Badveli","thumbnail":"https://cdn-images-1.medium.com/max/1017/1*kekDy_9eop0Au7trupOZCw.png","description":"\n<blockquote>Please note that this is <strong>not investment advice</strong> in any way and I do not recommend anyone to use this strategy to put their money. If anyone still does it, the responsibility is upon them and not in anyway on\u00a0me!</blockquote>\n<p>Ah\u2026 that elusive dream of making millions through crypto trading. After reading countless stories of people hitting big by just holding bitcoin, I too have dreamed about leaping into this mania. But, I didn\u2019t have enough money during the 2017 boom and when I had enough money, I thought there was no more growth left in the crypto space. Well, the pandemic changed that and the prices have skyrocketed again reaching all time highs. Once again, I missed the boat. Anyway, enough with the whining, let me tell you about the dumbest trading strategy that I\u2019ve backtested with the data from\u00a0Binance.</p>\n<h3>The Naive\u00a0Strategy</h3>\n<p>Here it goes. Once I describe my strategy, you\u2019ll get to understand why I\u2019m calling it <em>\u201cthe naive strategy.\u201d </em>For any trading strategy, the goal is to determine an entry point (where you buy the asset) and an exit point (where you sell the asset and close your position). This is if the trade is a long position and vice-versa if the trade is a short position.</p>\n<p>Now, the technical analysis gurus do this by looking at the candlestick data for patterns that might help them determine these entry/exit points and hopefully they get it right and maybe gain some profit on the trade. Similarly, the HODL (Hold Onto Dear Life) group just buys the asset and hopes the value of the asset increases in the long term. <em>Not much fun with this, but still made millions for a lot of\u00a0people.</em></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1017/1*kekDy_9eop0Au7trupOZCw.png\"><figcaption>Example trade for a naive strategy. Note that timeframes might not be accurate.</figcaption></figure><p>My strategy is to simply enter the trade at any point of time and exit the trade when your profit reaches a preset <em>target_profit_percentage </em>(say 1%). If the trade goes wrong and you start losing money, then close them at a <em>target_loss_percentage</em> controlled by you (say 5%). Sometimes, it might happen that the loss is so big that it will never be within the loss you specified. You have to handle these cases manually.</p>\n<blockquote>That\u2019s all there is to the strategy, except maybe adding a couple of additional considerations. Like, <em>\u201cHow long should you wait for profit before opening another trade?\u201d, \u201cShould you open just a single trade at one time or multiple trades?\u201d, \u201cHow do you choose the direction of trade i.e., short/long?\u201d, \u201cWill you invest the same amount of stake for every\u00a0trade?\u201d.</em>\n</blockquote>\n<p>You might be thinking, this is so stupid and I\u2019ll end up with all trades in loss and lose my entire money. But, let\u2019s see what happened.</p>\n<h3>Results\u2026 not so bad\u00a0maybe?</h3>\n<p>I have tested this strategy on <strong><em>BTC/USDT</em></strong> and <strong><em>ETH/USDT </em></strong>for a period of 2 years (Apr 20\u200a\u2014\u200aMar 21, Apr 21\u200a\u2014\u200aMar 22) separately, with a starting balance of $500 and a starting stake of $100 (20% of the balance). And the stake is dynamically adjusted according to the balance at hand, again as a percentage of the available balance, before opening a new trade. The below image shows the results obtained.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/647/1*qOQ3jVaAEVFSmHVC1ZYvsA.png\"><figcaption>Obtained results from the naive strategy on both the currency\u00a0pairs.</figcaption></figure><p>Quite surprisingly, even without closing the lossy trades manually, ETH/USDT during Apr 20\u200a\u2014\u200aMar 21 period with <strong>Long </strong>configuration achieved remarkably a <strong>+168% returns</strong>. Incredible, right?</p>\n<p>Not so fast, the same currency pair in the same period but with a <strong>Short</strong> configuration remarkably lost almost all of its value. This shows how inconsistent the returns are if we were to follow this strategy\u00a0blindly.</p>\n<p>The reason for the high returns in the former case is because the market itself has increased during that period and if we\u2019re following the market, then it\u2019s obvious we should end up with crazy returns. And since the configuration in the later case is against the market trend, it\u2019s natural to end up with such a huge loss. Just imagine always shorting when the market is going\u00a0up!</p>\n<p>One of the interesting things in the above results is BTC/USDT ending up in net positive returns even while shorting in the Apr 21\u200a\u2014\u200aMar22 period. This might\u2019ve been due to both upward and downward trends in this\u00a0market.</p>\n<h3>The Random\u00a0Strategy</h3>\n<p>Now, it made me wonder what if I select the short/long randomly with equal probabilities. It sounds crazy, right? Obviously, if you\u2019re picking randomly, then you\u2019ll end up with huge losses. But let\u2019s see what happened. Here are the results for the same period and the same configuration as above, but with the Random strategy.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/646/1*LavAyvWpgNBif9iadl5oJw.png\"><figcaption>Results obtained by following the random strategy.</figcaption></figure><p>Whoa! How did we end up in positive returns during the Apr 21\u200a\u2014\u200aMar 22 period for both currency pairs? I mean, what\u2019s going on here? Honestly, I\u2019ve got no clue as to why it worked like this. Maybe even though our predictions are wrong, somehow the market fluctuated so much that we ended up becoming right in a\u00a0way.</p>\n<h3>The Forecast\u00a0Strategy</h3>\n<p>On a final note, with the above results, I\u2019ve seriously believed that if we could predict the market direction with reasonably good accuracy, then maybe\u2026 just maybe we can end up with consistent year-on-year returns. And imagine, if we could automate this entire trading strategy, it is as if you\u2019ve got a passive income source for life. Or maybe not. I won\u2019t know until I try it fully\u2026 I mean I\u2019m not saying it will work for sure, but I\u2019m going to do it even while knowing it might\u00a0fail.</p>\n<p>I\u2019m currently trying to implement this strategy with an LSTM-based price forecasting model. Let\u2019s see if this lives up to my expectations or crush my dreams yet again\u00a0:)</p>\n<h3>Tools Used</h3>\n<ul>\n<li>\n<a href=\"https://python-binance.readthedocs.io/en/latest/\"><strong><em>python-binance</em></strong></a> package for retrieving the historical data in 1hr\u00a0candles.</li>\n<li>PyCharm IDE for coding in\u00a0python.</li>\n</ul>\n<p>That\u2019s it, I suppose. See you next\u00a0time!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fba2f11f5073\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://wire.insiderfinance.io/dumbest-crypto-trading-strategy-ever-fba2f11f5073\">Dumbest crypto trading strategy ever!</a> was originally published in <a href=\"https://wire.insiderfinance.io/\">InsiderFinance Wire</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<blockquote>Please note that this is <strong>not investment advice</strong> in any way and I do not recommend anyone to use this strategy to put their money. If anyone still does it, the responsibility is upon them and not in anyway on\u00a0me!</blockquote>\n<p>Ah\u2026 that elusive dream of making millions through crypto trading. After reading countless stories of people hitting big by just holding bitcoin, I too have dreamed about leaping into this mania. But, I didn\u2019t have enough money during the 2017 boom and when I had enough money, I thought there was no more growth left in the crypto space. Well, the pandemic changed that and the prices have skyrocketed again reaching all time highs. Once again, I missed the boat. Anyway, enough with the whining, let me tell you about the dumbest trading strategy that I\u2019ve backtested with the data from\u00a0Binance.</p>\n<h3>The Naive\u00a0Strategy</h3>\n<p>Here it goes. Once I describe my strategy, you\u2019ll get to understand why I\u2019m calling it <em>\u201cthe naive strategy.\u201d </em>For any trading strategy, the goal is to determine an entry point (where you buy the asset) and an exit point (where you sell the asset and close your position). This is if the trade is a long position and vice-versa if the trade is a short position.</p>\n<p>Now, the technical analysis gurus do this by looking at the candlestick data for patterns that might help them determine these entry/exit points and hopefully they get it right and maybe gain some profit on the trade. Similarly, the HODL (Hold Onto Dear Life) group just buys the asset and hopes the value of the asset increases in the long term. <em>Not much fun with this, but still made millions for a lot of\u00a0people.</em></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1017/1*kekDy_9eop0Au7trupOZCw.png\"><figcaption>Example trade for a naive strategy. Note that timeframes might not be accurate.</figcaption></figure><p>My strategy is to simply enter the trade at any point of time and exit the trade when your profit reaches a preset <em>target_profit_percentage </em>(say 1%). If the trade goes wrong and you start losing money, then close them at a <em>target_loss_percentage</em> controlled by you (say 5%). Sometimes, it might happen that the loss is so big that it will never be within the loss you specified. You have to handle these cases manually.</p>\n<blockquote>That\u2019s all there is to the strategy, except maybe adding a couple of additional considerations. Like, <em>\u201cHow long should you wait for profit before opening another trade?\u201d, \u201cShould you open just a single trade at one time or multiple trades?\u201d, \u201cHow do you choose the direction of trade i.e., short/long?\u201d, \u201cWill you invest the same amount of stake for every\u00a0trade?\u201d.</em>\n</blockquote>\n<p>You might be thinking, this is so stupid and I\u2019ll end up with all trades in loss and lose my entire money. But, let\u2019s see what happened.</p>\n<h3>Results\u2026 not so bad\u00a0maybe?</h3>\n<p>I have tested this strategy on <strong><em>BTC/USDT</em></strong> and <strong><em>ETH/USDT </em></strong>for a period of 2 years (Apr 20\u200a\u2014\u200aMar 21, Apr 21\u200a\u2014\u200aMar 22) separately, with a starting balance of $500 and a starting stake of $100 (20% of the balance). And the stake is dynamically adjusted according to the balance at hand, again as a percentage of the available balance, before opening a new trade. The below image shows the results obtained.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/647/1*qOQ3jVaAEVFSmHVC1ZYvsA.png\"><figcaption>Obtained results from the naive strategy on both the currency\u00a0pairs.</figcaption></figure><p>Quite surprisingly, even without closing the lossy trades manually, ETH/USDT during Apr 20\u200a\u2014\u200aMar 21 period with <strong>Long </strong>configuration achieved remarkably a <strong>+168% returns</strong>. Incredible, right?</p>\n<p>Not so fast, the same currency pair in the same period but with a <strong>Short</strong> configuration remarkably lost almost all of its value. This shows how inconsistent the returns are if we were to follow this strategy\u00a0blindly.</p>\n<p>The reason for the high returns in the former case is because the market itself has increased during that period and if we\u2019re following the market, then it\u2019s obvious we should end up with crazy returns. And since the configuration in the later case is against the market trend, it\u2019s natural to end up with such a huge loss. Just imagine always shorting when the market is going\u00a0up!</p>\n<p>One of the interesting things in the above results is BTC/USDT ending up in net positive returns even while shorting in the Apr 21\u200a\u2014\u200aMar22 period. This might\u2019ve been due to both upward and downward trends in this\u00a0market.</p>\n<h3>The Random\u00a0Strategy</h3>\n<p>Now, it made me wonder what if I select the short/long randomly with equal probabilities. It sounds crazy, right? Obviously, if you\u2019re picking randomly, then you\u2019ll end up with huge losses. But let\u2019s see what happened. Here are the results for the same period and the same configuration as above, but with the Random strategy.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/646/1*LavAyvWpgNBif9iadl5oJw.png\"><figcaption>Results obtained by following the random strategy.</figcaption></figure><p>Whoa! How did we end up in positive returns during the Apr 21\u200a\u2014\u200aMar 22 period for both currency pairs? I mean, what\u2019s going on here? Honestly, I\u2019ve got no clue as to why it worked like this. Maybe even though our predictions are wrong, somehow the market fluctuated so much that we ended up becoming right in a\u00a0way.</p>\n<h3>The Forecast\u00a0Strategy</h3>\n<p>On a final note, with the above results, I\u2019ve seriously believed that if we could predict the market direction with reasonably good accuracy, then maybe\u2026 just maybe we can end up with consistent year-on-year returns. And imagine, if we could automate this entire trading strategy, it is as if you\u2019ve got a passive income source for life. Or maybe not. I won\u2019t know until I try it fully\u2026 I mean I\u2019m not saying it will work for sure, but I\u2019m going to do it even while knowing it might\u00a0fail.</p>\n<p>I\u2019m currently trying to implement this strategy with an LSTM-based price forecasting model. Let\u2019s see if this lives up to my expectations or crush my dreams yet again\u00a0:)</p>\n<h3>Tools Used</h3>\n<ul>\n<li>\n<a href=\"https://python-binance.readthedocs.io/en/latest/\"><strong><em>python-binance</em></strong></a> package for retrieving the historical data in 1hr\u00a0candles.</li>\n<li>PyCharm IDE for coding in\u00a0python.</li>\n</ul>\n<p>That\u2019s it, I suppose. See you next\u00a0time!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fba2f11f5073\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://wire.insiderfinance.io/dumbest-crypto-trading-strategy-ever-fba2f11f5073\">Dumbest crypto trading strategy ever!</a> was originally published in <a href=\"https://wire.insiderfinance.io/\">InsiderFinance Wire</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["trading-system","trading","crypto","cryptocurrency"]},{"title":"Reinforcement Learning\u200a\u2014\u200a2","pubDate":"2022-05-06 15:04:19","link":"https://medium.com/@nikhilbadveli6/reinforcement-learning-2-8e525986b3df?source=rss-9059cc379720------2","guid":"https://medium.com/p/8e525986b3df","author":"Nikhil Badveli","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*nmO9L5lF0CQDFPudSjwK7A.jpeg","description":"\n<h3>Reinforcement Learning\u200a\u2014\u200a2</h3>\n<p>This is the second part in the RL series. In the previous part, we have learnt about the basics of RL and implemented a simple Q-learning solution for the cartpole balancing problem.</p>\n<p>Now, in this post we will learn about solving the same problem using a bit more advanced approach called <strong><em>Deep Q-learning</em></strong>. This technique was originally published by researchers from DeepMind\u00b9, a subsidiary of Google that\u2019s behind <a href=\"https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DWXuK6gekU1Y&amp;psig=AOvVaw3RcHYTBvan9JnxCc3BVnBm&amp;ust=1651935584598000&amp;source=images&amp;cd=vfe&amp;ved=0CA0QjhxqFwoTCICl4OORy_cCFQAAAAAdAAAAABAP\">the famous AlphaGo project</a>. The authors achieved state-of-the-art results in six of the Atari-2600 arcade games and has surpassed humans in three of\u00a0them.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nmO9L5lF0CQDFPudSjwK7A.jpeg\"><figcaption>A documentary on AlphaGo. Image taken from DataDrivenInvestor.</figcaption></figure><p><em>(I\u2019m going to assume that you are already familiar with neural networks and how awesome they are. If not, I would highly suggest to at least learn the\u00a0basics.)</em></p>\n<h3>An attempt at introducing Deep Q-learning</h3>\n<p>The entire premise of this approach is to utilize the power of deep neural networks for an RL problem. Specifically, there are two common ways it is done. One is <strong>Q-network</strong> and the other is called <strong>Policy network</strong>. The idea is to use the Universal function approximation property of neural networks to learn the Q-function for the former and the policy function for the\u00a0latter.</p>\n<p>For the cartpole balancing problem we\u2019re going to build a Policy network using keras library in python. Essentially, the inputs for this network are state values (4 for this problem) and the outputs are action values (2 for this problem). And there will be a couple of hidden layers in between. All layers are fully-connected. The below code builds the\u00a0network.</p>\n<a href=\"https://medium.com/media/506e3e83f1c4765d9046ce1e978a01be/href\">https://medium.com/media/506e3e83f1c4765d9046ce1e978a01be/href</a><p>Now, one of the key components of this technique is the <strong><em>replay buffer</em></strong>. It is just a memory unit that stores the history of the past states, the action taken, the reward obtained and the next state for each of those past states. You can think of this like a list that holds the tuple <em>(state, action, reward, next_state, done)</em> for each of its elements. This is important because the network can only learn from past experience and so we\u2019re collecting it before we use it to\u00a0train.</p>\n<a href=\"https://medium.com/media/03ddf9b40c69078e256149e7d166da6c/href\">https://medium.com/media/03ddf9b40c69078e256149e7d166da6c/href</a><p>The next step is the training part of the network. A couple of questions that might arise here are, <em>\u201cHow frequently should we train the network and for how many epochs?\u201d, \u201cHow do we transform the values in the replay buffer as inputs and targets to the network?\u201d. </em>Read on to find out\u00a0:)</p>\n<p>Since the replay buffer is of fixed size and as we cannot train the network on the entire experience accumulated, we will randomly sample the buffer for a fixed <em>mini_batch_size</em> (say 32) and then use these to train the network in every time step. Each time we will train only for one epoch. (probably not\u00a0ideal)</p>\n<a href=\"https://medium.com/media/a9777b43e8822e2e7e39d560eb4f0d87/href\">https://medium.com/media/a9777b43e8822e2e7e39d560eb4f0d87/href</a><p>In the above code, there\u2019s a for loop that shows how to convert the replay buffer tuple into actual inputs and targets to be passed to the <em>model.fit() </em>function. Here the target values are between 0 and\u00a01.</p>\n<p>We can put together everything until now in a python class. You can find this in this <a href=\"https://github.com/NIkhilbadveli/Artificial_Intelligence/blob/master/5.%20Reinforcement%20Learning/deep_q_network.ipynb\">jupyter notebook</a> along with the rest of the\u00a0code.</p>\n<h3>Training the\u00a0network</h3>\n<p>Now, using the above created class we can instantiate a new Deep Q-Network object and then it can be used by the agent to predict what action to take in any given state. <em>I\u2019m assuming you already know about episodes and time steps\u00a0:)</em></p>\n<a href=\"https://medium.com/media/120cf60395bd8272786de7f1f1cf9946/href\">https://medium.com/media/120cf60395bd8272786de7f1f1cf9946/href</a><p>One thing to note is that we are using an <em>epsilon greedy strategy</em> to balance the exploration and exploitation of the state space. And the epsilon factor is used in a decaying fashion to decrease the exploration in the later\u00a0stage.</p>\n<h3>Testing the\u00a0network</h3>\n<p>While testing the network, we will always exploit i.e., use our knowledge of the state space so far and make the network predictions which are used to take an\u00a0action.</p>\n<a href=\"https://medium.com/media/f0151f5a3577efb8f6adc65dc90b4990/href\">https://medium.com/media/f0151f5a3577efb8f6adc65dc90b4990/href</a><p>The problem is considered to be solved if the average score per episode is more than 195. This is to be sure that the network solved the problem by learning and not through sheer\u00a0luck.</p>\n<h3>Plotting the\u00a0results</h3>\n<p>Finally, a good old plot to understand how the network evolved as it gained more experience through playing in a number of episodes. Here\u2019s the code for\u00a0it.</p>\n<a href=\"https://medium.com/media/917c33903ae1dbcbdf8aaed4f4a9845f/href\">https://medium.com/media/917c33903ae1dbcbdf8aaed4f4a9845f/href</a><p>And here\u2019s the actual plot generated after training the policy network created\u00a0above.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/384/1*q2ikTaq9shs3C2wstzG8SQ.png\"><figcaption>Avg. score vs No. of\u00a0episodes</figcaption></figure><p><em>Sorry about having no legend, no title and axis labels\u00a0:(</em></p>\n<h3>References</h3>\n<ol><li>Playing Atari with Deep Reinforcement Learning <a href=\"https://arxiv.org/pdf/1312.5602.pdf\">https://arxiv.org/pdf/1312.5602.pdf</a>\n</li></ol>\n<p><a href=\"https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762\">My Journey Into Deep Q-Learning with Keras and Gym</a></p>\n<p>Also, read my previous post on Reinforcement Learning.</p>\n<p><a href=\"https://medium.com/@nikhilbadveli6/reinforcement-learning-1-79cdec3599f9\">Reinforcement Learning\u200a\u2014\u200a1</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8e525986b3df\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Reinforcement Learning\u200a\u2014\u200a2</h3>\n<p>This is the second part in the RL series. In the previous part, we have learnt about the basics of RL and implemented a simple Q-learning solution for the cartpole balancing problem.</p>\n<p>Now, in this post we will learn about solving the same problem using a bit more advanced approach called <strong><em>Deep Q-learning</em></strong>. This technique was originally published by researchers from DeepMind\u00b9, a subsidiary of Google that\u2019s behind <a href=\"https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DWXuK6gekU1Y&amp;psig=AOvVaw3RcHYTBvan9JnxCc3BVnBm&amp;ust=1651935584598000&amp;source=images&amp;cd=vfe&amp;ved=0CA0QjhxqFwoTCICl4OORy_cCFQAAAAAdAAAAABAP\">the famous AlphaGo project</a>. The authors achieved state-of-the-art results in six of the Atari-2600 arcade games and has surpassed humans in three of\u00a0them.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nmO9L5lF0CQDFPudSjwK7A.jpeg\"><figcaption>A documentary on AlphaGo. Image taken from DataDrivenInvestor.</figcaption></figure><p><em>(I\u2019m going to assume that you are already familiar with neural networks and how awesome they are. If not, I would highly suggest to at least learn the\u00a0basics.)</em></p>\n<h3>An attempt at introducing Deep Q-learning</h3>\n<p>The entire premise of this approach is to utilize the power of deep neural networks for an RL problem. Specifically, there are two common ways it is done. One is <strong>Q-network</strong> and the other is called <strong>Policy network</strong>. The idea is to use the Universal function approximation property of neural networks to learn the Q-function for the former and the policy function for the\u00a0latter.</p>\n<p>For the cartpole balancing problem we\u2019re going to build a Policy network using keras library in python. Essentially, the inputs for this network are state values (4 for this problem) and the outputs are action values (2 for this problem). And there will be a couple of hidden layers in between. All layers are fully-connected. The below code builds the\u00a0network.</p>\n<a href=\"https://medium.com/media/506e3e83f1c4765d9046ce1e978a01be/href\">https://medium.com/media/506e3e83f1c4765d9046ce1e978a01be/href</a><p>Now, one of the key components of this technique is the <strong><em>replay buffer</em></strong>. It is just a memory unit that stores the history of the past states, the action taken, the reward obtained and the next state for each of those past states. You can think of this like a list that holds the tuple <em>(state, action, reward, next_state, done)</em> for each of its elements. This is important because the network can only learn from past experience and so we\u2019re collecting it before we use it to\u00a0train.</p>\n<a href=\"https://medium.com/media/03ddf9b40c69078e256149e7d166da6c/href\">https://medium.com/media/03ddf9b40c69078e256149e7d166da6c/href</a><p>The next step is the training part of the network. A couple of questions that might arise here are, <em>\u201cHow frequently should we train the network and for how many epochs?\u201d, \u201cHow do we transform the values in the replay buffer as inputs and targets to the network?\u201d. </em>Read on to find out\u00a0:)</p>\n<p>Since the replay buffer is of fixed size and as we cannot train the network on the entire experience accumulated, we will randomly sample the buffer for a fixed <em>mini_batch_size</em> (say 32) and then use these to train the network in every time step. Each time we will train only for one epoch. (probably not\u00a0ideal)</p>\n<a href=\"https://medium.com/media/a9777b43e8822e2e7e39d560eb4f0d87/href\">https://medium.com/media/a9777b43e8822e2e7e39d560eb4f0d87/href</a><p>In the above code, there\u2019s a for loop that shows how to convert the replay buffer tuple into actual inputs and targets to be passed to the <em>model.fit() </em>function. Here the target values are between 0 and\u00a01.</p>\n<p>We can put together everything until now in a python class. You can find this in this <a href=\"https://github.com/NIkhilbadveli/Artificial_Intelligence/blob/master/5.%20Reinforcement%20Learning/deep_q_network.ipynb\">jupyter notebook</a> along with the rest of the\u00a0code.</p>\n<h3>Training the\u00a0network</h3>\n<p>Now, using the above created class we can instantiate a new Deep Q-Network object and then it can be used by the agent to predict what action to take in any given state. <em>I\u2019m assuming you already know about episodes and time steps\u00a0:)</em></p>\n<a href=\"https://medium.com/media/120cf60395bd8272786de7f1f1cf9946/href\">https://medium.com/media/120cf60395bd8272786de7f1f1cf9946/href</a><p>One thing to note is that we are using an <em>epsilon greedy strategy</em> to balance the exploration and exploitation of the state space. And the epsilon factor is used in a decaying fashion to decrease the exploration in the later\u00a0stage.</p>\n<h3>Testing the\u00a0network</h3>\n<p>While testing the network, we will always exploit i.e., use our knowledge of the state space so far and make the network predictions which are used to take an\u00a0action.</p>\n<a href=\"https://medium.com/media/f0151f5a3577efb8f6adc65dc90b4990/href\">https://medium.com/media/f0151f5a3577efb8f6adc65dc90b4990/href</a><p>The problem is considered to be solved if the average score per episode is more than 195. This is to be sure that the network solved the problem by learning and not through sheer\u00a0luck.</p>\n<h3>Plotting the\u00a0results</h3>\n<p>Finally, a good old plot to understand how the network evolved as it gained more experience through playing in a number of episodes. Here\u2019s the code for\u00a0it.</p>\n<a href=\"https://medium.com/media/917c33903ae1dbcbdf8aaed4f4a9845f/href\">https://medium.com/media/917c33903ae1dbcbdf8aaed4f4a9845f/href</a><p>And here\u2019s the actual plot generated after training the policy network created\u00a0above.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/384/1*q2ikTaq9shs3C2wstzG8SQ.png\"><figcaption>Avg. score vs No. of\u00a0episodes</figcaption></figure><p><em>Sorry about having no legend, no title and axis labels\u00a0:(</em></p>\n<h3>References</h3>\n<ol><li>Playing Atari with Deep Reinforcement Learning <a href=\"https://arxiv.org/pdf/1312.5602.pdf\">https://arxiv.org/pdf/1312.5602.pdf</a>\n</li></ol>\n<p><a href=\"https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762\">My Journey Into Deep Q-Learning with Keras and Gym</a></p>\n<p>Also, read my previous post on Reinforcement Learning.</p>\n<p><a href=\"https://medium.com/@nikhilbadveli6/reinforcement-learning-1-79cdec3599f9\">Reinforcement Learning\u200a\u2014\u200a1</a></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8e525986b3df\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","alphago","reinforcement-learning","deep-q-learning"]},{"title":"Reinforcement Learning\u200a\u2014\u200a1","pubDate":"2022-04-26 15:07:48","link":"https://medium.com/@nikhilbadveli6/reinforcement-learning-1-79cdec3599f9?source=rss-9059cc379720------2","guid":"https://medium.com/p/79cdec3599f9","author":"Nikhil Badveli","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*ZpYxRG-BnvaR7A-6","description":"\n<h3>Reinforcement Learning\u200a\u2014\u200a1</h3>\n<p>In this series of posts, we will learn about reinforcement learning (RL)by implementing basic projects that can illustrate RL concepts in a easy to understand manner.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*ZpYxRG-BnvaR7A-6\"><figcaption>Photo by <a href=\"https://unsplash.com/@agk42?utm_source=medium&amp;utm_medium=referral\">Alex Knight</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>This is the first post in this series and we will try to solve <strong>the cartpole balancing problem</strong> that\u2019s often used as a hello world example of\u00a0RL.</p>\n<h3>About Reinforcement Learning</h3>\n<p>Before we proceed, let\u2019s give a brief overview of the basic components of an RL problem. Every RL setting will have the following parts in some form or the\u00a0other.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/617/1*9cEBv2oXvT5BbYjPyeZrXA.png\"><figcaption>Illustration taken from Lil\u2019Log\u00a0blog</figcaption></figure><ul>\n<li>\n<strong>Agent</strong>\u00a0:- This is the entity that performs actions and interacts with its surroundings and receives a corresponding reward associated with the action. In the cartpole example, the agent is the box (cart) that has a pole on top of it and it can move left or\u00a0right.</li>\n<li>\n<strong>Environment\u00a0</strong>:-<strong> </strong>As the name implies, this is the space (doesn\u2019t have to be physical) where an agent interacts with. Sometimes the environment is completely determined by a model and a lot of times there\u2019s no specific model that governs the behavior of the environment. In the cartpole example, the environment can be thought of as a 1-dimensional line\u00a0segment.</li>\n<li>\n<strong>Reward</strong>\u00a0:- This is the function that determines the reward the agent should receive for each of its actions. For example, in a game of chess, the reward could be \u20181\u2019 when the agent wins and \u20180\u2019 when it\u00a0loses.</li>\n</ul>\n<p>In simple words, the goal of RL is to make the agent learn a policy function which is used to determine the best possible action at every state so as to maximize the discounted cumulative future reward. As such, this is a very simple, generalized framework and yet incredibly powerful enough to solve unimaginably complex problems such as Protein\u00a0folding.</p>\n<p>Anyways, I\u2019m not going to explain all the things about RL. But, let me point out to useful resources (check references section at the end) which can give you much more complete\u00a0picture.</p>\n<h3>Q-learning</h3>\n<p>Now that you\u2019ve got a basic understanding of RL, let me elaborate the cartpole problem. As I\u2019ve said in the previous section, the agent is a box and the environment is a 1-D line segment and the goal of the box is to balance the pole as long as possible without letting it\u00a0fall.</p>\n<p>The actions the agent can take are limited to going left or going right by a fixed amount. And so, the reward is \u20180\u2019 whenever the pole angle is greater than 12 degrees or the box is out of bounds and \u20181\u2019 otherwise.</p>\n<p>To solve this particular problem, we will use something called Q-learning. It uses a function called Q function which takes a state (representation of the environment) and an action as the input, and spits out the discounted cumulative future reward. Luckily for us, this function can be represented recursively in time\u00a0steps.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ajQ89I72kdiv0sSVo_FwCg.png\"><figcaption>Q-function expressed recursively in timesteps.</figcaption></figure><p>It is by exploiting this property along with the nice capability of not requiring a model of the environment for learning, Q-learning is made possible.</p>\n<p>In practice, the Q function is implemented as a lookup table with states in rows and actions in columns. And this table is filled dynamically while learning.</p>\n<h3>Coding part</h3>\n<p>Now comes the part where everyone is interested in. We will use OpenAI\u2019s Gym framework to solve this problem. If you don\u2019t know what is Gym and how it works, read this article\u00b3. Here\u2019s the code to get the cartpole environment in\u00a0Gym.</p>\n<a href=\"https://medium.com/media/104b6f17d0b4daf918061eb4a8fc1eef/href\">https://medium.com/media/104b6f17d0b4daf918061eb4a8fc1eef/href</a><p>Before we proceed further, there\u2019s something called state space and action space that need to be explained. Essentially, they represent a set of all possible states and a set of all possible actions respectively. For the cartpole problem, the state is represented using four variables namely\u200a\u2014\u200aCart Position, Cart Velocity, Pole Angle and Pole Angular Velocity. And the actions\u200a\u2014\u200aLeft and right are represented by \u2018-1\u2019 and \u2018+1\u2019 values respectively.</p>\n<p>Since we\u2019re using a Q-table which cannot hold infinite values, we\u2019ve to discretize the state space into bins and assign any observed state into one of these bins. As there are limits to the values of these four variables, we will uniformly divide them into the same number of\u00a0bins.</p>\n<p>Here\u2019s the code for creating a\u00a0Q-table.</p>\n<a href=\"https://medium.com/media/7f5d782dff22c9c7e476247344290aed/href\">https://medium.com/media/7f5d782dff22c9c7e476247344290aed/href</a><p>Next step is where the actual learning happens. Here\u2019s the code for the\u00a0same.</p>\n<a href=\"https://medium.com/media/70ea924a2fdf1569327c4ce662b8f874/href\">https://medium.com/media/70ea924a2fdf1569327c4ce662b8f874/href</a><p>There are a lot of things going on in the above code. Let me break down. The function takes <strong><em>discount factor</em></strong><em> </em>and <strong><em>epsilon</em></strong><em> </em>as parameters to use while learning. We also give a maximum number of episodes (an episode is a single game of play until the agent fails and environment get reset) to continue learning. Each episode can run until an arbitrary number of timesteps depending on the performance of the\u00a0agent.</p>\n<p>Roughly, the pseudo code for the above implementation is as\u00a0follows:</p>\n<ul>\n<li>loop through\u00a0episodes</li>\n<li>reset the environment</li>\n<li>loop until\u00a0done</li>\n<li>sample from action space with epsilon probability and take action based on the populated Q-table for the remaining probability.</li>\n<li>if episode is not terminated then update the Q-table, else update the\u00a0reward.</li>\n</ul>\n<p><em>Sorry about the bad pseudocode. Will use an online generator next time\u00a0:)</em></p>\n<p>The outer <em>for </em>loop is used to loop through each episode and accumulate the reward while the inner <em>while</em> loop takes a random action with a probability of epsilon. During each time step, it is checked if the episode is terminated (if <em>done </em>is True). In this case the total score is updated along with the average score per episode. The Cartpole problem is considered as solved if this score is more than or equal to\u00a0150.</p>\n<p>Now, just put together everything and don\u2019t forget to close the environment after learning is completed.</p>\n<a href=\"https://medium.com/media/49d7ec7834c44f98099c7cb474a5697d/href\">https://medium.com/media/49d7ec7834c44f98099c7cb474a5697d/href</a><p>And voila! your very first RL project is done. Of course this project is only meant to be an introduction to the RL world and there is so much else to learn. We will slowly build the other concepts in the upcoming\u00a0posts.</p>\n<p>All the code in this article is heavily inspired from this article\u00b9. And you can read more about getting started with RL\u00b2 and OpenAI\u00a0Gym\u00b3.</p>\n<h3>References</h3>\n<ol>\n<li><a href=\"https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5\">https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5</a></li>\n<li><a href=\"https://gordicaleksa.medium.com/how-to-get-started-with-reinforcement-learning-rl-4922fafeaf8c\">https://gordicaleksa.medium.com/how-to-get-started-with-reinforcement-learning-rl-4922fafeaf8c</a></li>\n<li><a href=\"https://gym.openai.com/docs/\">https://gym.openai.com/docs/</a></li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=79cdec3599f9\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Reinforcement Learning\u200a\u2014\u200a1</h3>\n<p>In this series of posts, we will learn about reinforcement learning (RL)by implementing basic projects that can illustrate RL concepts in a easy to understand manner.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*ZpYxRG-BnvaR7A-6\"><figcaption>Photo by <a href=\"https://unsplash.com/@agk42?utm_source=medium&amp;utm_medium=referral\">Alex Knight</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>This is the first post in this series and we will try to solve <strong>the cartpole balancing problem</strong> that\u2019s often used as a hello world example of\u00a0RL.</p>\n<h3>About Reinforcement Learning</h3>\n<p>Before we proceed, let\u2019s give a brief overview of the basic components of an RL problem. Every RL setting will have the following parts in some form or the\u00a0other.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/617/1*9cEBv2oXvT5BbYjPyeZrXA.png\"><figcaption>Illustration taken from Lil\u2019Log\u00a0blog</figcaption></figure><ul>\n<li>\n<strong>Agent</strong>\u00a0:- This is the entity that performs actions and interacts with its surroundings and receives a corresponding reward associated with the action. In the cartpole example, the agent is the box (cart) that has a pole on top of it and it can move left or\u00a0right.</li>\n<li>\n<strong>Environment\u00a0</strong>:-<strong> </strong>As the name implies, this is the space (doesn\u2019t have to be physical) where an agent interacts with. Sometimes the environment is completely determined by a model and a lot of times there\u2019s no specific model that governs the behavior of the environment. In the cartpole example, the environment can be thought of as a 1-dimensional line\u00a0segment.</li>\n<li>\n<strong>Reward</strong>\u00a0:- This is the function that determines the reward the agent should receive for each of its actions. For example, in a game of chess, the reward could be \u20181\u2019 when the agent wins and \u20180\u2019 when it\u00a0loses.</li>\n</ul>\n<p>In simple words, the goal of RL is to make the agent learn a policy function which is used to determine the best possible action at every state so as to maximize the discounted cumulative future reward. As such, this is a very simple, generalized framework and yet incredibly powerful enough to solve unimaginably complex problems such as Protein\u00a0folding.</p>\n<p>Anyways, I\u2019m not going to explain all the things about RL. But, let me point out to useful resources (check references section at the end) which can give you much more complete\u00a0picture.</p>\n<h3>Q-learning</h3>\n<p>Now that you\u2019ve got a basic understanding of RL, let me elaborate the cartpole problem. As I\u2019ve said in the previous section, the agent is a box and the environment is a 1-D line segment and the goal of the box is to balance the pole as long as possible without letting it\u00a0fall.</p>\n<p>The actions the agent can take are limited to going left or going right by a fixed amount. And so, the reward is \u20180\u2019 whenever the pole angle is greater than 12 degrees or the box is out of bounds and \u20181\u2019 otherwise.</p>\n<p>To solve this particular problem, we will use something called Q-learning. It uses a function called Q function which takes a state (representation of the environment) and an action as the input, and spits out the discounted cumulative future reward. Luckily for us, this function can be represented recursively in time\u00a0steps.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ajQ89I72kdiv0sSVo_FwCg.png\"><figcaption>Q-function expressed recursively in timesteps.</figcaption></figure><p>It is by exploiting this property along with the nice capability of not requiring a model of the environment for learning, Q-learning is made possible.</p>\n<p>In practice, the Q function is implemented as a lookup table with states in rows and actions in columns. And this table is filled dynamically while learning.</p>\n<h3>Coding part</h3>\n<p>Now comes the part where everyone is interested in. We will use OpenAI\u2019s Gym framework to solve this problem. If you don\u2019t know what is Gym and how it works, read this article\u00b3. Here\u2019s the code to get the cartpole environment in\u00a0Gym.</p>\n<a href=\"https://medium.com/media/104b6f17d0b4daf918061eb4a8fc1eef/href\">https://medium.com/media/104b6f17d0b4daf918061eb4a8fc1eef/href</a><p>Before we proceed further, there\u2019s something called state space and action space that need to be explained. Essentially, they represent a set of all possible states and a set of all possible actions respectively. For the cartpole problem, the state is represented using four variables namely\u200a\u2014\u200aCart Position, Cart Velocity, Pole Angle and Pole Angular Velocity. And the actions\u200a\u2014\u200aLeft and right are represented by \u2018-1\u2019 and \u2018+1\u2019 values respectively.</p>\n<p>Since we\u2019re using a Q-table which cannot hold infinite values, we\u2019ve to discretize the state space into bins and assign any observed state into one of these bins. As there are limits to the values of these four variables, we will uniformly divide them into the same number of\u00a0bins.</p>\n<p>Here\u2019s the code for creating a\u00a0Q-table.</p>\n<a href=\"https://medium.com/media/7f5d782dff22c9c7e476247344290aed/href\">https://medium.com/media/7f5d782dff22c9c7e476247344290aed/href</a><p>Next step is where the actual learning happens. Here\u2019s the code for the\u00a0same.</p>\n<a href=\"https://medium.com/media/70ea924a2fdf1569327c4ce662b8f874/href\">https://medium.com/media/70ea924a2fdf1569327c4ce662b8f874/href</a><p>There are a lot of things going on in the above code. Let me break down. The function takes <strong><em>discount factor</em></strong><em> </em>and <strong><em>epsilon</em></strong><em> </em>as parameters to use while learning. We also give a maximum number of episodes (an episode is a single game of play until the agent fails and environment get reset) to continue learning. Each episode can run until an arbitrary number of timesteps depending on the performance of the\u00a0agent.</p>\n<p>Roughly, the pseudo code for the above implementation is as\u00a0follows:</p>\n<ul>\n<li>loop through\u00a0episodes</li>\n<li>reset the environment</li>\n<li>loop until\u00a0done</li>\n<li>sample from action space with epsilon probability and take action based on the populated Q-table for the remaining probability.</li>\n<li>if episode is not terminated then update the Q-table, else update the\u00a0reward.</li>\n</ul>\n<p><em>Sorry about the bad pseudocode. Will use an online generator next time\u00a0:)</em></p>\n<p>The outer <em>for </em>loop is used to loop through each episode and accumulate the reward while the inner <em>while</em> loop takes a random action with a probability of epsilon. During each time step, it is checked if the episode is terminated (if <em>done </em>is True). In this case the total score is updated along with the average score per episode. The Cartpole problem is considered as solved if this score is more than or equal to\u00a0150.</p>\n<p>Now, just put together everything and don\u2019t forget to close the environment after learning is completed.</p>\n<a href=\"https://medium.com/media/49d7ec7834c44f98099c7cb474a5697d/href\">https://medium.com/media/49d7ec7834c44f98099c7cb474a5697d/href</a><p>And voila! your very first RL project is done. Of course this project is only meant to be an introduction to the RL world and there is so much else to learn. We will slowly build the other concepts in the upcoming\u00a0posts.</p>\n<p>All the code in this article is heavily inspired from this article\u00b9. And you can read more about getting started with RL\u00b2 and OpenAI\u00a0Gym\u00b3.</p>\n<h3>References</h3>\n<ol>\n<li><a href=\"https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5\">https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5</a></li>\n<li><a href=\"https://gordicaleksa.medium.com/how-to-get-started-with-reinforcement-learning-rl-4922fafeaf8c\">https://gordicaleksa.medium.com/how-to-get-started-with-reinforcement-learning-rl-4922fafeaf8c</a></li>\n<li><a href=\"https://gym.openai.com/docs/\">https://gym.openai.com/docs/</a></li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=79cdec3599f9\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["q-learning","reinforcement-learning","cartpole","openai-gym"]},{"title":"New and Exciting trends in AI","pubDate":"2022-04-17 21:05:00","link":"https://medium.com/@nikhilbadveli6/new-and-exciting-trends-in-ai-b2a83250d1c6?source=rss-9059cc379720------2","guid":"https://medium.com/p/b2a83250d1c6","author":"Nikhil Badveli","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*_qDUZwVhdU7sLzi9","description":"\n<p>The field of Artificial Intelligence is probably the most active and the pace of new developments is nothing short of marvel. Here are some latest and greatest trends that I\u2019ve observed recently.</p>\n<h3>Modular AI\u200a\u2014\u200acreating close to human intelligence</h3>\n<p>Meta\u2019s AI head and deep learning pioneer Yann LeCun is keen on the future of AI. He says the current methods of training models is very inefficient in terms of cost, energy and data requirements. And he\u2019s big on \u201cSelf supervised learning\u201d which doesn\u2019t need that much labelled\u00a0data.</p>\n<p>He proposed that, for AI to come close to human level intelligence, it needs to learn world models and furthermore a sort of memory is required. He is saying, such a modular architecture is the key to achieving the ultimate\u00a0AGI.</p>\n<h3>Analog AI\u200a\u2014\u200amore efficient way of training\u00a0models</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*_qDUZwVhdU7sLzi9\"><figcaption>Photo by <a href=\"https://unsplash.com/@rioryan?utm_source=medium&amp;utm_medium=referral\">Ryan</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>According to this IBM Research\u2019s <a href=\"https://analog-ai-demo.mybluemix.net/hardware\">article</a>,</p>\n<blockquote>Analog AI delivers radical performance improvements by combining compute and memory in a single device, eliminating the von Neumann bottleneck.</blockquote>\n<p>In lay terms, the current method of training neural networks is inefficient in terms of energy consumption since there\u2019s a lot of movement of data between the RAM and the CPU. With Phase-Changed Memory (PCM), the computation takes place where the memory is and as such reduces the power requirements drastically along with faster computation speed.</p>\n<h3>Explainable AI\u200a\u2014\u200anot a black box\u00a0anymore</h3>\n<p>One of the major drawbacks of using neural networks to solve a learning problem is\u2026 you can\u2019t really tell what made the model work, at least not entirely. Although with CNNs, we an visualize the neuron activations and see what patterns activate each layer, the same can\u2019t be said about other\u00a0models.</p>\n<p>And this is especially important in sensitive applications such as self-driving car, to be able to explain why a certain decision is made is one of the key challenges in autonomous driving. As a result, there has been a shift in the industry and academia to encourage more explainable models.</p>\n<h3>Model Size\u200a\u2014\u200anot the only thing that\u00a0matters</h3>\n<p>Ever since GPT-3 with its unprecedented 175 billion parameter model has shown immense success, popularity there has been an uptick in ever increasing model sizes. For example, Gopher (280B) from DeepMind to rival GPT-3, Megtron-Turing NLG (530B) etc., This is because, it was concluded that the only way to increase model performance is to give more data or increase the model\u00a0size.</p>\n<p>Debunking this conclusion, DeepMind has released a much smaller model <a href=\"https://towardsdatascience.com/a-new-ai-trend-chinchilla-70b-greatly-outperforms-gpt-3-175b-and-gopher-280b-408b9b4510\"><strong>Chinchilla</strong> </a>with only 70 billion parameters that could significantly outperform GPT-3 (175B) and even Gopher (280B). Their findings essentially created the new law of model scaling\u200a\u2014\u200awhich states that number of tokens during model training is just as important as the number of parameters of the\u00a0model.</p>\n<h3>OpenAI\u2019s DALL-E 2\u200a\u2014\u200afeels like\u00a0magic</h3>\n<p>The successor to the original <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a> model, that can turn any words you give it to you into a picture that imaginatively captures what has been asked of it. Check some of the cool stuff it <a href=\"https://www.lesswrong.com/posts/r99tazGiLgzqFX7ka/playing-with-dall-e-2\">generated</a>. Without going into the technical intricacies of the model, let\u2019s just say this was all made possible thanks to the all encompassing Transformer model and the subsequent extensions of\u00a0it.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*SqgK9HKC0S2Kirdy\"><figcaption>Photo by <a href=\"https://unsplash.com/@shaikhulud?utm_source=medium&amp;utm_medium=referral\">Maxim Tolchinskiy</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Personally, I feel like there\u2019s an incredible potential for it to become mainstream in one of our day-to-day social media products or maybe this might the key to generating all of those assets that are needed to make the <em>Metaverse</em> possible.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b2a83250d1c6\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>The field of Artificial Intelligence is probably the most active and the pace of new developments is nothing short of marvel. Here are some latest and greatest trends that I\u2019ve observed recently.</p>\n<h3>Modular AI\u200a\u2014\u200acreating close to human intelligence</h3>\n<p>Meta\u2019s AI head and deep learning pioneer Yann LeCun is keen on the future of AI. He says the current methods of training models is very inefficient in terms of cost, energy and data requirements. And he\u2019s big on \u201cSelf supervised learning\u201d which doesn\u2019t need that much labelled\u00a0data.</p>\n<p>He proposed that, for AI to come close to human level intelligence, it needs to learn world models and furthermore a sort of memory is required. He is saying, such a modular architecture is the key to achieving the ultimate\u00a0AGI.</p>\n<h3>Analog AI\u200a\u2014\u200amore efficient way of training\u00a0models</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*_qDUZwVhdU7sLzi9\"><figcaption>Photo by <a href=\"https://unsplash.com/@rioryan?utm_source=medium&amp;utm_medium=referral\">Ryan</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>According to this IBM Research\u2019s <a href=\"https://analog-ai-demo.mybluemix.net/hardware\">article</a>,</p>\n<blockquote>Analog AI delivers radical performance improvements by combining compute and memory in a single device, eliminating the von Neumann bottleneck.</blockquote>\n<p>In lay terms, the current method of training neural networks is inefficient in terms of energy consumption since there\u2019s a lot of movement of data between the RAM and the CPU. With Phase-Changed Memory (PCM), the computation takes place where the memory is and as such reduces the power requirements drastically along with faster computation speed.</p>\n<h3>Explainable AI\u200a\u2014\u200anot a black box\u00a0anymore</h3>\n<p>One of the major drawbacks of using neural networks to solve a learning problem is\u2026 you can\u2019t really tell what made the model work, at least not entirely. Although with CNNs, we an visualize the neuron activations and see what patterns activate each layer, the same can\u2019t be said about other\u00a0models.</p>\n<p>And this is especially important in sensitive applications such as self-driving car, to be able to explain why a certain decision is made is one of the key challenges in autonomous driving. As a result, there has been a shift in the industry and academia to encourage more explainable models.</p>\n<h3>Model Size\u200a\u2014\u200anot the only thing that\u00a0matters</h3>\n<p>Ever since GPT-3 with its unprecedented 175 billion parameter model has shown immense success, popularity there has been an uptick in ever increasing model sizes. For example, Gopher (280B) from DeepMind to rival GPT-3, Megtron-Turing NLG (530B) etc., This is because, it was concluded that the only way to increase model performance is to give more data or increase the model\u00a0size.</p>\n<p>Debunking this conclusion, DeepMind has released a much smaller model <a href=\"https://towardsdatascience.com/a-new-ai-trend-chinchilla-70b-greatly-outperforms-gpt-3-175b-and-gopher-280b-408b9b4510\"><strong>Chinchilla</strong> </a>with only 70 billion parameters that could significantly outperform GPT-3 (175B) and even Gopher (280B). Their findings essentially created the new law of model scaling\u200a\u2014\u200awhich states that number of tokens during model training is just as important as the number of parameters of the\u00a0model.</p>\n<h3>OpenAI\u2019s DALL-E 2\u200a\u2014\u200afeels like\u00a0magic</h3>\n<p>The successor to the original <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a> model, that can turn any words you give it to you into a picture that imaginatively captures what has been asked of it. Check some of the cool stuff it <a href=\"https://www.lesswrong.com/posts/r99tazGiLgzqFX7ka/playing-with-dall-e-2\">generated</a>. Without going into the technical intricacies of the model, let\u2019s just say this was all made possible thanks to the all encompassing Transformer model and the subsequent extensions of\u00a0it.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*SqgK9HKC0S2Kirdy\"><figcaption>Photo by <a href=\"https://unsplash.com/@shaikhulud?utm_source=medium&amp;utm_medium=referral\">Maxim Tolchinskiy</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Personally, I feel like there\u2019s an incredible potential for it to become mainstream in one of our day-to-day social media products or maybe this might the key to generating all of those assets that are needed to make the <em>Metaverse</em> possible.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b2a83250d1c6\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["trends","towards-ai","towards-data-science","dall-e2"]},{"title":"Random thoughts on AI","pubDate":"2022-03-20 22:20:06","link":"https://medium.com/@nikhilbadveli6/random-thoughts-on-ai-2a5e74aec5df?source=rss-9059cc379720------2","guid":"https://medium.com/p/2a5e74aec5df","author":"Nikhil Badveli","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*ruj3FpxwwjCzA9890Hdw7g.jpeg","description":"\n<p>Artificial General Intelligence (AGI) is the ultimate holy grail of the field of AI. But, are researchers doing enough on this front? Is it even correct to try to create an AGI? Should we just focus on the way things are currently, for example building neural networks/models for a narrow problem\u00a0domain?</p>\n<p>In the book \u201cThousand brains theory of Intelligence\u201d by Jeff Hawkins, the author describes an interesting take on this topic. On the contrary to everyone\u2019s belief that if we create an AGI, it will ultimately lead the end of humanity as we know it, Jeff Hawkins says it might actually be really beneficial for the advancement of humans as a\u00a0species.</p>\n<p>He says that machines do not have motives, it\u2019s only humans that have it. And if these powerful machines were to fall in the wrong hands, then that might cause the destruction we expected. But the machines themselves are harmless.</p>\n<p>I think he\u2019s right. Maybe there\u2019s a way to create an intelligent being without all the defects of humans. We just don\u2019t know how\u2026\u00a0yet.</p>\n<p>But, I\u2019ve always wondered how would such a system look like. How is it that our brain is able to achieve this remarkable feat? Is it simply the mind boggling number of neural connections in our brain? Or is it a mixture of these things (Embodied AI, 2D neural network layers, non-linear neural connections, transfer learning through evolution, memory, knowledge graph)\u00a0somehow?</p>\n<p>When I read Alan Turing\u2019s Imitation Game paper, the thing that stood out for me was not the \u201cTuring test\u201d. Instead, I was fascinated to see Turing trying to describe how someone can possibly go about creating true AI. He said, we\u2019ve to build a system that works like teaching something to a human child. I\u2019m sure he never would\u2019ve thought the world will choose Neural Networks as the preferred architecture to achieve this\u00a0task.</p>\n<p>How can I use all of these ideas together and create a general purpose learning machine? How do I code reference frames as described in the book mentioned above?</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ruj3FpxwwjCzA9890Hdw7g.jpeg\"></figure><p>This is part -1 of the series of my random thoughts on AI. Next topics include:-</p>\n<p><strong><em>Building human like computer vision\u00a0system.</em></strong></p>\n<p><strong><em>Reference frames and Embodied\u00a0AI.</em></strong></p>\n<p><strong><em>Two dimensional Neural Networks.</em></strong></p>\n<p><strong><em>Non-linear neural connections.</em></strong></p>\n<p><strong><em>Memory and Knowledge graph in\u00a0AI.</em></strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2a5e74aec5df\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Artificial General Intelligence (AGI) is the ultimate holy grail of the field of AI. But, are researchers doing enough on this front? Is it even correct to try to create an AGI? Should we just focus on the way things are currently, for example building neural networks/models for a narrow problem\u00a0domain?</p>\n<p>In the book \u201cThousand brains theory of Intelligence\u201d by Jeff Hawkins, the author describes an interesting take on this topic. On the contrary to everyone\u2019s belief that if we create an AGI, it will ultimately lead the end of humanity as we know it, Jeff Hawkins says it might actually be really beneficial for the advancement of humans as a\u00a0species.</p>\n<p>He says that machines do not have motives, it\u2019s only humans that have it. And if these powerful machines were to fall in the wrong hands, then that might cause the destruction we expected. But the machines themselves are harmless.</p>\n<p>I think he\u2019s right. Maybe there\u2019s a way to create an intelligent being without all the defects of humans. We just don\u2019t know how\u2026\u00a0yet.</p>\n<p>But, I\u2019ve always wondered how would such a system look like. How is it that our brain is able to achieve this remarkable feat? Is it simply the mind boggling number of neural connections in our brain? Or is it a mixture of these things (Embodied AI, 2D neural network layers, non-linear neural connections, transfer learning through evolution, memory, knowledge graph)\u00a0somehow?</p>\n<p>When I read Alan Turing\u2019s Imitation Game paper, the thing that stood out for me was not the \u201cTuring test\u201d. Instead, I was fascinated to see Turing trying to describe how someone can possibly go about creating true AI. He said, we\u2019ve to build a system that works like teaching something to a human child. I\u2019m sure he never would\u2019ve thought the world will choose Neural Networks as the preferred architecture to achieve this\u00a0task.</p>\n<p>How can I use all of these ideas together and create a general purpose learning machine? How do I code reference frames as described in the book mentioned above?</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ruj3FpxwwjCzA9890Hdw7g.jpeg\"></figure><p>This is part -1 of the series of my random thoughts on AI. Next topics include:-</p>\n<p><strong><em>Building human like computer vision\u00a0system.</em></strong></p>\n<p><strong><em>Reference frames and Embodied\u00a0AI.</em></strong></p>\n<p><strong><em>Two dimensional Neural Networks.</em></strong></p>\n<p><strong><em>Non-linear neural connections.</em></strong></p>\n<p><strong><em>Memory and Knowledge graph in\u00a0AI.</em></strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=2a5e74aec5df\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["towards-ai","embodied-ai","random-thoughts","towards-data-science","aritificial-intelligence"]},{"title":"World\u2019s First Hotel\u00a0: Nishiyama Onsen keiunkan","pubDate":"2020-04-14 15:52:59","link":"https://medium.com/@nikhilbadveli6/worlds-first-hotel-nishiyama-onsen-keiunkan-422fad49255b?source=rss-9059cc379720------2","guid":"https://medium.com/p/422fad49255b","author":"Nikhil Badveli","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*M2_nceTuud5-_FVBfoV7yA.jpeg","description":"\n<h3>World\u2019s First Hotel\u00a0: Nishiyama Onsen\u00a0keiunkan</h3>\n<p><strong>Today, we have widespread hotel chains like Marriott, Hyatt serving both the average customer and the luxury customer. Surprisingly, hotels have a rich history dating back to 705 AD, originating from an unexpected country.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*M2_nceTuud5-_FVBfoV7yA.jpeg\"></figure><p>You must have probably read the weird name in the title and wondered what is it\u00a0about.</p>\n<p>Apparently, it is the name of the world\u2019s first hotel. \u201c<strong>Nishiyama Onsen keiunkan</strong>\u201d<strong> </strong>was founded in Hayakawa, Yamanashi prefecture, Japan in 705 AD by Fujiwara Mahito operating under a company called\u00a0Yushima.</p>\n<p>This is a typical hot spring hotel you can find in Japan even in the current era. The name of this hotel could be translated to \u201c<strong>Western Mountain Hot Springs of Keiun\u00a0era</strong>\u201d.</p>\n<p>The company operating this hotel is perhaps the second oldest company in the world, still in operation after 52 generations of the same family amounting to 1300\u00a0years.</p>\n<p>Only in 2011 did Guinness World Records recognized this hotel as the oldest hotel in the world. I don\u2019t know why they took so long\u00a0:)</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/1*v9p5BmjsuqAbiY6rXNfJ_w.jpeg\"></figure><p>Similar to most of the onsens in Japan, Nishiyama Onsen keiunkan also lied at the foot of the mountains known as\u00a0<strong>Akaishi</strong>.</p>\n<p>Since the foundation of this hotel all of its hot water was sourced directly from the nearby Hakuho\u00a0Springs</p>\n<p>The founder of this hotel Fujiwara Mahito-san was a son of an aide to the 38th Emperor of Japan, <strong>Emperor\u00a0Tenji</strong>.</p>\n<p>Nishiyama Onsen keiunkan has 37 rooms, a splendid restaurant (kaiseki), and a great <strong>moon-viewing</strong> platform. The rooms are furnished by Tatami mats and classic art while the staff wears nibu-shiki kimonos.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=422fad49255b\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>World\u2019s First Hotel\u00a0: Nishiyama Onsen\u00a0keiunkan</h3>\n<p><strong>Today, we have widespread hotel chains like Marriott, Hyatt serving both the average customer and the luxury customer. Surprisingly, hotels have a rich history dating back to 705 AD, originating from an unexpected country.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*M2_nceTuud5-_FVBfoV7yA.jpeg\"></figure><p>You must have probably read the weird name in the title and wondered what is it\u00a0about.</p>\n<p>Apparently, it is the name of the world\u2019s first hotel. \u201c<strong>Nishiyama Onsen keiunkan</strong>\u201d<strong> </strong>was founded in Hayakawa, Yamanashi prefecture, Japan in 705 AD by Fujiwara Mahito operating under a company called\u00a0Yushima.</p>\n<p>This is a typical hot spring hotel you can find in Japan even in the current era. The name of this hotel could be translated to \u201c<strong>Western Mountain Hot Springs of Keiun\u00a0era</strong>\u201d.</p>\n<p>The company operating this hotel is perhaps the second oldest company in the world, still in operation after 52 generations of the same family amounting to 1300\u00a0years.</p>\n<p>Only in 2011 did Guinness World Records recognized this hotel as the oldest hotel in the world. I don\u2019t know why they took so long\u00a0:)</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/1*v9p5BmjsuqAbiY6rXNfJ_w.jpeg\"></figure><p>Similar to most of the onsens in Japan, Nishiyama Onsen keiunkan also lied at the foot of the mountains known as\u00a0<strong>Akaishi</strong>.</p>\n<p>Since the foundation of this hotel all of its hot water was sourced directly from the nearby Hakuho\u00a0Springs</p>\n<p>The founder of this hotel Fujiwara Mahito-san was a son of an aide to the 38th Emperor of Japan, <strong>Emperor\u00a0Tenji</strong>.</p>\n<p>Nishiyama Onsen keiunkan has 37 rooms, a splendid restaurant (kaiseki), and a great <strong>moon-viewing</strong> platform. The rooms are furnished by Tatami mats and classic art while the staff wears nibu-shiki kimonos.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=422fad49255b\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["oldest-hotel","facts","first-hotel","interesting","world"]},{"title":"Backup and Restore files from Firebase Storage","pubDate":"2020-02-27 14:43:11","link":"https://medium.com/@nikhilbadveli6/backup-and-restore-files-from-firebase-storage-77a0121023b2?source=rss-9059cc379720------2","guid":"https://medium.com/p/77a0121023b2","author":"Nikhil Badveli","thumbnail":"https://cdn-images-1.medium.com/max/850/1*WEk97wvmTz-DsH2DNoooFA.png","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/850/1*WEk97wvmTz-DsH2DNoooFA.png\"></figure><p>This article introduces you to <strong>Firebase Storage</strong>, another one of Google\u2019s free cloud storage options. It is different from <strong>Firebase Realtime Database </strong>as the former one is for storing any files while the latter is for storing a structured JSON\u00a0data.</p>\n<p><strong>Pre-requisites:</strong></p>\n<ul>\n<li>Knowing how to connect Android Studio project to Firebase. If you don\u2019t know, <a href=\"https://firebase.google.com/docs/android/setup#assistant\">read this\u00a0article</a>.</li>\n<li>Getting proper permissions to read and write to external\u00a0storage.</li>\n</ul>\n<p>I\u2019m not going to write about making a file picker and then choosing the file you want to upload. Here, I\u2019ll just show you how to upload and download a file at a certain location in your local\u00a0storage.</p>\n<p>First, add correct dependencies by copy pasting the following lines in your <strong>build.gradle (Module: app)</strong> file. (<em>this is the latest version as of\u00a0writing)</em></p>\n<pre>implementation 'com.google.firebase:firebase-storage:19.1.1'</pre>\n<p><strong>Uploading a\u00a0file</strong></p>\n<p>I\u2019m taking a text file for the sake of example, you can use the same process for any file. First, get reference to the FirebaseStorage instance.</p>\n<ul>\n<li>Then, you\u2019ve to get an uri from the file using the function <strong><em>Uri.fromFile</em></strong> which takes a file object as it\u2019s\u00a0input.</li>\n<li>To create file object for your file, first get the path to local storage by calling <strong><em>getFilesDir().getAbsolutePath()</em></strong><em> </em>and then you can add the corresponding path to your\u00a0file.</li>\n<li>After this all you\u2019ve to do is create an upload task by calling <strong><em>putFile</em></strong><em> </em>method on the storage reference object previously created.</li>\n</ul>\n<a href=\"https://medium.com/media/4af37d85c4c618aca58dad2beabd5fe2/href\">https://medium.com/media/4af37d85c4c618aca58dad2beabd5fe2/href</a><p>And finally, to know the status of upload task, just add listeners\u200a\u2014\u200aone for failure and one for success. You can show a toast or print a log to notify in these cases. You can do a lot more if you need more customization like adding metadata, monitoring the upload progress and pausing, resuming, cancelling. <a href=\"https://firebase.google.com/docs/storage/android/upload-files\">Refer this firebase documentation to know about\u00a0these.</a></p>\n<p><strong>Downloading the previously uploaded\u00a0file</strong></p>\n<p>This is just as easier as uploading. There are multiple ways of downloading\u200a\u2014\u200ausing the storage path, using download URL. I\u2019m going to show you the first method. For this, you\u2019ve to know the path of the file stored in FirebaseStorage.</p>\n<a href=\"https://medium.com/media/64614353fad5d0eea642d0328fd88642/href\">https://medium.com/media/64614353fad5d0eea642d0328fd88642/href</a><p>As you can see from the above code, most of it is almost same as uploading except when you\u2019re creating a download task, you\u2019ll be giving a file object instead of an URI. To know more about other ways of downloading <a href=\"https://firebase.google.com/docs/storage/android/download-files\">refer\u00a0here</a>.</p>\n<p>That\u2019s it for today. Some of the common problems people face when implementing this would\u00a0be:</p>\n<ul>\n<li>Not setting up the dependencies correctly</li>\n<li>Not asking for run time permissions for build \u2265 Marshmallow</li>\n<li>Using incorrect path when creating the file\u00a0object.</li>\n</ul>\n<p>As with all things in programming, only perseverance can help you in finding your error and hopefully debugging it.</p>\n<p>P.S: This is my first article. Please suggest me on what can be improved further. Thanks in advance\u00a0:)</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=77a0121023b2\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/850/1*WEk97wvmTz-DsH2DNoooFA.png\"></figure><p>This article introduces you to <strong>Firebase Storage</strong>, another one of Google\u2019s free cloud storage options. It is different from <strong>Firebase Realtime Database </strong>as the former one is for storing any files while the latter is for storing a structured JSON\u00a0data.</p>\n<p><strong>Pre-requisites:</strong></p>\n<ul>\n<li>Knowing how to connect Android Studio project to Firebase. If you don\u2019t know, <a href=\"https://firebase.google.com/docs/android/setup#assistant\">read this\u00a0article</a>.</li>\n<li>Getting proper permissions to read and write to external\u00a0storage.</li>\n</ul>\n<p>I\u2019m not going to write about making a file picker and then choosing the file you want to upload. Here, I\u2019ll just show you how to upload and download a file at a certain location in your local\u00a0storage.</p>\n<p>First, add correct dependencies by copy pasting the following lines in your <strong>build.gradle (Module: app)</strong> file. (<em>this is the latest version as of\u00a0writing)</em></p>\n<pre>implementation 'com.google.firebase:firebase-storage:19.1.1'</pre>\n<p><strong>Uploading a\u00a0file</strong></p>\n<p>I\u2019m taking a text file for the sake of example, you can use the same process for any file. First, get reference to the FirebaseStorage instance.</p>\n<ul>\n<li>Then, you\u2019ve to get an uri from the file using the function <strong><em>Uri.fromFile</em></strong> which takes a file object as it\u2019s\u00a0input.</li>\n<li>To create file object for your file, first get the path to local storage by calling <strong><em>getFilesDir().getAbsolutePath()</em></strong><em> </em>and then you can add the corresponding path to your\u00a0file.</li>\n<li>After this all you\u2019ve to do is create an upload task by calling <strong><em>putFile</em></strong><em> </em>method on the storage reference object previously created.</li>\n</ul>\n<a href=\"https://medium.com/media/4af37d85c4c618aca58dad2beabd5fe2/href\">https://medium.com/media/4af37d85c4c618aca58dad2beabd5fe2/href</a><p>And finally, to know the status of upload task, just add listeners\u200a\u2014\u200aone for failure and one for success. You can show a toast or print a log to notify in these cases. You can do a lot more if you need more customization like adding metadata, monitoring the upload progress and pausing, resuming, cancelling. <a href=\"https://firebase.google.com/docs/storage/android/upload-files\">Refer this firebase documentation to know about\u00a0these.</a></p>\n<p><strong>Downloading the previously uploaded\u00a0file</strong></p>\n<p>This is just as easier as uploading. There are multiple ways of downloading\u200a\u2014\u200ausing the storage path, using download URL. I\u2019m going to show you the first method. For this, you\u2019ve to know the path of the file stored in FirebaseStorage.</p>\n<a href=\"https://medium.com/media/64614353fad5d0eea642d0328fd88642/href\">https://medium.com/media/64614353fad5d0eea642d0328fd88642/href</a><p>As you can see from the above code, most of it is almost same as uploading except when you\u2019re creating a download task, you\u2019ll be giving a file object instead of an URI. To know more about other ways of downloading <a href=\"https://firebase.google.com/docs/storage/android/download-files\">refer\u00a0here</a>.</p>\n<p>That\u2019s it for today. Some of the common problems people face when implementing this would\u00a0be:</p>\n<ul>\n<li>Not setting up the dependencies correctly</li>\n<li>Not asking for run time permissions for build \u2265 Marshmallow</li>\n<li>Using incorrect path when creating the file\u00a0object.</li>\n</ul>\n<p>As with all things in programming, only perseverance can help you in finding your error and hopefully debugging it.</p>\n<p>P.S: This is my first article. Please suggest me on what can be improved further. Thanks in advance\u00a0:)</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=77a0121023b2\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["download","upload","android-app-development","firebase","android"]}]}